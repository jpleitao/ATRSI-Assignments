\documentclass[9pt,journal,compsoc]{IEEEtran}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{array}
\usepackage{algorithmic}
\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{mydef}{Definition}


\begin{document}

\title{\huge Artificial Neural Networks in Feature Extraction: A Review}

% Author information
\author{
	\IEEEauthorblockN{Joaquim LeitÃ£o\IEEEauthorrefmark{1}}
	
	\IEEEauthorblockA{\IEEEauthorrefmark{1}jpleitao@dei.uc.pt \\ CISUC, Department of Informatics Engineering, Univesity of Coimbra, Portugal}
}

% The paper headers
\markboth{Real Time Learning in Intelligent Systems, 2016-2017}{Real Time Learning in Intelligent Systems, 2016-2017}


\IEEEtitleabstractindextext{
\begin{abstract}
Data representation plays a crucial role in the overall performance of machine learning algorithms, since successfully implementing learning algorithms that work in some (typically high-dimensional) input spaces is far from being an easy task. Therefore, machine learning algorithms must learn and apply proper techniques to reduce the dimensionality of their input spaces.

Dimensionality reduction is composed by two major classes of methods: On the one side there are Feature Extraction methods, which propose a new set of features by performing transformations on a given feature set. Feature Extraction algorithms can further be classified as either linear or nonlinear, depending on the nature of the performed transformations. On the other side, Feature Selection methods (ideally) select the best subset of features a provided feature set.

The current document intends to survey the literature on the application of artificial neural networks in feature extraction tasks, providing a summarised overview of this research topic.
\end{abstract}

\begin{IEEEkeywords}
	Feature Extraction, Artificial Neural Networks.
\end{IEEEkeywords}
}


% make the title area
\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{M}{achine} Learning (ML) can shortly be defined as a sub-field of computer science which allows a computer to learn a given task without being explicitly programmed to do such task. Since computers are not directly told how to solve a given task, ML approaches aim to learn the task to be solved by collecting and analysing high volumes of data on this task. Most popular ML tasks involve performing some sort of classification or categorisation of collected data, or making predictions based on historical collected information.

In such tasks choosing the right representation for the collected information is of extreme importance and highly conditions the computer's ability to learn the desired task. Indeed, \emph{real-life} data tends to be very high-dimensional, where most learning algorithms tend to perform poorly.

As a result, proper dimensionality reduction techniques must be adopted in order to reduce the dimensionality of the input spaces. Dimensionality reduction is composed by two major classes of methods: \emph{Feature Extraction} (FE) and \emph{Feature Selection} (FS). While FE methods propose a new set of features by performing transformations on a given feature set, FS methods (ideally) select the best subset of features a provided feature set.

The current document intends to cover FE techniques proposed in the literature, providing a literature review on this topic. Furthermore, only the application of Artificial Neural Networks in FE is intended to be covered. Therefore, additional referenced techniques applied for this purpose (such as Principal Components Analysis) will not be covered.

The remainder of this document is organised as follows: Section \ref{sec:background} introduces the need to perform FE in Intelligent Systems. Section \ref{sec:anns_feature_extraction} presents the most popular and referenced techniques exploring the use of Artificial Neural Networks for FE purposes. Finally, section \ref{sec:conclusion} concludes the document.

\section{Background}\label{sec:background}

The current section addresses the importance of Feature Extraction in Intelligent Systems. In the literature this concept is often presented in the sequel of the well-known curse of dimensionality phenomenon, as a collection of methods that allow a reduction in the problem's dimensionality.

\subsection{Curse of Dimensionality}
\label{curse_dimensionality}

The curse of dimensionality refers to how certain learning algorithms may perform poorly when confronted with high-dimensional data. In many Pattern Recognition (PR) problems, low-dimension feature spaces may not allow for a good separation of the data, leading to low-accuracy classifications. By considering more features, a feature space where an hyperplane that perfectly separates the (training) data can be determined with a resulting increase in classification accuracy.

Nonetheless it has been demonstrated that, as dimensionality increases, the amount of training data needed to accurately generalise the produced classifier grows exponentially. In addition, a small \emph{training samples-to-features} ratio may also degrade performance \cite{jain2000statistical}.

A practical implication of the curse of dimensionality is that a system designer should select a small number of features when confronted with a limited training dataset. A general accepted practice is to keep the number of training samples at least ten times higher than the dimensionality: $n / d > 10$ \cite{jain2000statistical}.

\subsection{Dimensionality Reduction}

In light of the discussion conducted in \ref{curse_dimensionality}, a common interest in the vast majority of MPR applications is to keep the number of features as small as possible.

Feature Extraction and Feature Selection techniques are the most cited and recognised methods for achieving dimensionality reduction in PR problems. Feature Extraction algorithms propose a new set of features by performing transformations on a given feature set. On their turn, Feature Selection algorithms (ideally) select the best subset of features a provided feature set. As pointed out by Jain \emph{et al.} \cite{jain2000statistical}, feature extraction is typically applied before feature selection\footnote{The idea is to start by defining a new feature set based on the original one, and then select the features that, hopefully, allow a complete separation of the training data.}.

The main issues in dimensionality reduction, namely in feature extraction and selection, are related with the choice of criterion function and the appropriate dimension for the reduced feature space.

\section{Neural Networks in Feature Extraction}\label{sec:anns_feature_extraction}

Artificial Neural Networks (ANNs) have been extensively applied in the field of machine learning, in both classification and regression tasks. Because of their self-learning and trained characteristics, ANNs have also been successfully applied in FE tasks.

Considering the simplest form of ANNs, feedforward networks, FE can be obtained in the output of each hidden layer: the application of the neurons' activation function to the layer's input produces an output that can be interpreted as a set of new features \cite{jain2000statistical}. Depending on the neurons' activation function this new set of features can be a linear or nonlinear combinations of the features in the input feature set (usually is nonlinear).

In the remainder of the current section the most popular and referenced ANN architectures applied in FE tasks are intended to be covered. Such architectures include \emph{Feedforward} (namely \emph{Stacked Autoencoders}), \emph{Convolutional}, \emph{Recurrent}, \emph{Radial Basis}, \emph{Restricted Boltzmann Machines} and \emph{Self-Organising Maps}\cite{jain2000statistical, masci2011stacked, kvascev2012radial, fabius2014variational, cho2014learning, marchi2017deep}.

\subsection{Feedforward}

A feedforward ANN is one of the simplest examples of a neural network. The network is composed of several layers of neurons, where neurons in each layer are connected to neurons in the next layer: that is, connections between the units do not form a cycle. In the beginning of the current section feedforward networks were used as a motivation for the application of ANNs in FE tasks.

In its simplest forms, FE can be achieved through single or multi-layer feedforward ANNs by training the network to reconstruct the input signal in a conventional way, using a backpropagation algorithm (or other popular alternatives). This form of ANN training is considered unsupervised, as only unlabelled data is being used.

In an opposite view, a multi-layer feedforward network may be trained to perform a classification task. As multiple layers are being considered, the output of the last hidden layer (which will provide the input to the output layer) can be considered as the extracted features form the provided input sample. Networks trained with this approach have been proven to suffer from well-known \emph{vanishing gradient} problems\cite{nielsen2015neural}.

In both scenarios the FE process is learned automatically in the early layers of the final network. In recent years, namely with advances in the field of deep learning, approaches of the first type tend to be more popular and widely used.

In this sense, training of feedforward networks for FE purposes usually comprises two steps: \emph{Pre-training} and \emph{Fine tuning}.

During pre-training a series of hidden layers are trained independently in an unsupervised way: Groups of 3-layer feedforward nets (one for each hidden layer of the final network) are trained to reconstruct their respective inputs at the output layer. The idea is to make each network learn an encoder and a decoder\footnote{The encoder is learned from the input to the hidden layer, while the decoder is learned from the hidden to the output layer.}. In this process, the first network is trained with the input data; the second network is trained with the output of the first network's encoder, and so on. This process is illustrated in figure \ref{barata_saes}.

When pre-training is complete, the encoders are stacked together. Due to this last step, ANNs developed in this way are often referred to as \emph{Stacked autoencoders}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{stacked_autoencoders.png}
	\caption{Stacking autoencoders. Taken from \cite{barata_saes_presentation}.}
	\label{barata_saes}
\end{figure}

After the described pre-training, a fine tuning step is commonly carried out by considering an additional layer (usually with a softmax activation function) and training the network with backpropagation (or other algorithm), to lightly adjust the weights of the pre-trained layers.

At this moment it is important to address one point in the discussion: based on what has been presented, autoenconders may simply learn the identity function for both the encoder and the decoder, which is not desired. To prevent this, certain activation functions can be forced, producing \emph{regularized autoencoders} such as sparse autoencoders \cite{vincent2010stacked}, denoising autoencoders \cite{ngiam2011optimization} and contractive autoencoders \cite{rifai2011contractive}.

Feedforward networks, namely \emph{Stacked autoencoders}, can be applied for Feature Extraction purposes in a series of applications, including image processing and classification and speech recognition \cite{vincent2010stacked, lu2013speech}, just to name a few.

\subsection{Convolutional Neural Networks}

Convolutional Neural Networks (CNN) are ANNs designed to process data that comes in the form of multiple arrays, such as an image of a sequence of 1D signals. CNNs are based on four key ideas that take advantage of the properties of natural signals: local connections, shared weights, pooling and multi-layer usage \cite{lecun2015deep}. Similarly to the feedforward case, CNNs can be used to automatically extract features and perform classification tasks.

The architecture of a CNN comprises several stages: In the first few two types of layers are alternated: \emph{convolutional} and \emph{pooling} - In the literature it is common to refer to a pair of convolutional and pooling as one hidden layer of the CNN, or a \emph{slice}. Following these sequences of alternating convolutional and pooling layers, a fully connected layer is considered in order to produce the network's output. Figure \ref{convolutional_neural_net} presents a conceptual example of a CNN.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{Convolutional_Neural_Net.png}
	\caption{Conceptual example of a Convolutional Neural Network. Taken from \cite{arel2010deep}.}
	\label{convolutional_neural_net}
\end{figure}

In short, units in convolutional layers are organised in feature maps, where for each map a subset of units from one hidden layer serves as input to the next hidden layer. Like in other ANN architectures, each input sample has an associated weight. The same weight vector must be applied to the neurons in a single depth slice.

The logic behind the convolutional layer is to perform, for each map in a given hidden layer, a convolution of the neurons' weights with the input volume. The result of this local weighted sum is then passed through a non-linear activation function, such as a \emph{ReLU}\footnote{Rectified Linear Unit.}.

Because of the mentioned weight sharing principle, it is common to refer to the sets of weights as a \emph{filter} or \emph{kernel} that is applied to the input. For this reason, the convolutional layer is often described as consisting in the application of several filters to the input (resulting from the different feature maps considered). It can also be said that the role of this layer is to detect local conjunctions of features \cite{lecun2015deep}.

In a different view, units in the pooling layers enforce a form of non-linear down-sampling, by seeking to merge semantically similar features into one \cite{lecun2015deep}. Several non-linear functions have been proposed to this purpose, a typical pooling function is the \emph{max pooling}, which computes the maximum of a local set of units in one (or few) feature map.

From a feature extraction and dimensionality reduction point of view, the described sequence of convolutional and pooling layers is responsible for reducing the dimensionality of the provided input set. In some scenarios a normalisation layer may be considered; however, as pointed out in \cite{cs2312017convolutional}, this practice has fallen out of favor because its contribution has been shown to be minimal.

In the vast majority of the scenarios, CNNs are applied in classification problems, and for that reason additional fully-connected layers are considered where an high-level reasoning is performed. Regarding the network's training, the application of a backpropagation gradient through a CNN is similar to regular deep networks, allowing all the weights in all the layers and filters to be trained.

Convolutional Neural Networks have been extensively applied for Feature Extraction purposes, with the most sounding applications in fields such as image\cite{lawrence1997face, krizhevsky2012imagenet}, video\cite{karpathy2014large} and speech processing and recognition\cite{lecun1995convolutional, abdel2012applying}, natural language processing\cite{hu2014convolutional} and analysis of other sequences of 1D signals. CNNs were also applied to develop an artificial intelligence for the board game Go\cite{clark2014teaching, maddison2014move}

\subsection{Recurrent Neural Networks}

Recurrent Neural Networks (RNN) are ANNs designed to process sequential information. Unlike in Feedforward networks, where each layer can only be connected to the next one, forming a directed acyclic graph, connections in a RNN can form directed cycles. That is, in a RNN one neuron layer can be connected to a previous layer, or itself. Figure \ref{recurrent_ann} presents a RNN with one hidden layer.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.2]{recurrent_ann.png}
	\caption{Illustration of a Recurrent Neural Network. Taken from \cite{deep2015recurrent}.}
	\label{recurrent_ann}
\end{figure}

Two distinct types of recurrence can be considered in RNN: If the outputs of a given layer are connected to the inputs of the same layer, then \emph{internal recurrence} is being considered; however, if the output layer is connected to the input layer, \emph{external recurrence} is being considered. Current RNN architectures tend to consider \emph{internal recurrence} rather than \emph{external recurrence}.

RNNs can be seen as structures with \emph{memory}, allowing them to capture information about what previous computations. This is the main reason why RNNs are so popular and effective when dealing with sequential data, such as time series. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps.

Similarly to Feedforward networks, several activation functions can be considered in the neurons of a RNN, of which sigmoid and softmax activation functions are examples.

Studying the literature in the topic of RNNs several architectures for RNNs can be found. the most popular architectures proposed in the literature include \emph{Elman}\cite{elman1990finding,elman1991distributed} and \emph{Jordan}\cite{jordan1997serial} networks. Because of space restrictions, a detailed description of the different structures will not be presented. The reader is directed to the cited works for a more detailed coverage.

With respect to the training of this networks, \emph{Backpropagation Through Time} (BPTT) is among the most referenced and known algorithms applied to RNN. BPTT is defined as an adaptation of the popular backpropagation\footnote{A detailed explanation on how the backpropagation method works can be found in \cite{nielsen2015neural}.} training method applied in feedforward neural networks. This method proposes an \emph{"unfolding"} of the recurrences in the network, as presented in figure \ref{recurrent_ann_training}. 

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{recurrent_ann_training.jpg}
	\caption{Illustration of a RecurrentNeural Network. Taken from \cite{deep2015recurrent}.}
	\label{recurrent_ann_training}
\end{figure}

Applying this technique, a feedforward neural network is obtained, and the backpropagation algorithm can be applied. During the training process the BPTT algorithm progressively unfolds the network, leading to a continuous increase in the size of the network. In theory, BPTT does not define a limit for the unfolding, although approaches such as \emph{Truncated BPTT} have been proposed to achieve this by defining a fixed number of layers in the network.

Other alternatives to these methods can also be identified in the literature, of which \emph{Kalman Filters}\cite{kalman1960new}, \emph{Real-Time Recurrent Learning} (RTRL)\cite{williams1989learning} and \emph{Long Short-Term Memory (LSTM)}\cite{hochreiter1997long}.

The main feature of the RTRL algorithm has to due with its mathematical formulation: the derivatives are explicitly computed, and therefore, no unfolding is performed. By its turn, LSTM avoids the well-known vanishing gradient problems and introduces a distinct architecture, where units are implemented in blocks containing gates used to control the flow of information into or out of their memory. Again, due to size restrictions on the current paper, the reader is deferred to the cited literature for a more in-depth coverage of these techinques.

Recurrent Neural Networks have been extensively applied for Feature Extraction purposes, with the most sounding applications in fields such as image classification \cite{graves2009offline}; time series prediction, namely in EEG signal prediction \cite{prasad2014deep}, speech recognition\cite{lee2015high}, among others. The work of Lee and Tashev\cite{lee2015high}, focusing on speech recognition, applies RNNs in automatic high-level feature representation, which can be considered as a FE task. Indeed, a transformation of raw input data to a representation that can be effectively exploited in machine learning tasks is carried out by a RNN in this work.

\subsection{Restricted Boltzmann Machines}

\emph{Restricted Boltzmann Machines} (RBM)\cite{smolensky1986information} can be defined as a variant of Bolztamnn Machines\footnote{Which, by its turn, is a type of stochastic Recurrent Neural Network and Markov Random Field.} \cite{ackley1985learning} with the restriction that the neurons must form a bipartite graph.

RBM comprehend two types of neurons, also called units: \emph{visible} and \emph{hidden} units. Neurons in the visible unit are connected to the hidden unit through a set of weights in a fully-connected way. The hidden units are used to model the data distribution:

Briefly, the following idea is adopted: Values in the visible unit are known (they can be the original input data) while values in the hidden units are not known. Therefore, the task for a RBM is to estimate these values, and this is accomplished by learning a probabilistic model to describe these values. In the literature covering this topic, this is the same as stating that the values $P(h_{j} | v)$ are going to be learned, where $h_{j}$ is related with one neuron in the hidden unit, while $v$ is related with the values in the visible unit (which are known).

The values of $P(h_{j} | v)$ are learned for a given value of $W$, which includes the weights of the connections between the two units and the bias of the neurons in the RBM. The training of a RBM consists in updating the weights and the bias of the neurons, as referenced in \cite{cai2012feature}.

Regarding Feature Extraction, the connections from the visible to the hidden units can be seen as an encoder, and the connection in the opposite direction as a decoder \cite{ribeiro2017deep}. This is illustrated in figure \ref{restricted_boltzmann_machines}. Therefore, if one considers the expected value of the hidden units, this can be viewed as a set of features extracted from the input data \cite{cai2012feature}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{restricted_boltzmann_machine.png}
	\caption{Restricted Boltzmann Machine. Taken from \cite{ribeiro2017deep}.}
	\label{restricted_boltzmann_machines}
\end{figure}

Resembling the common practices with feedforward networks, namely with autoencoders, RBM can also be stacked together forming \emph{Belief Networks}, or \emph{Deep Belief Networks} (DBN) \cite{hinton2009deep} if a deep architecture is considered.

RBM and DBN have been applied in several problems of distinct natures such as stock price prediction\cite{cai2012feature} and predictions from EEG data \cite{hajinoroozi2015feature}, just to name a few.

\subsection{Self-Organising Maps}

In Self-Organising Maps (SOM), another type of ANN used for Non-Linear Feature Extraction \cite{jain2000statistical, hira2015review}, a low-dimension discretized representation of the input space is produced. Such representation is called a map.

A SOM consists of neurons with associated weight vectors (with the same dimension as the input). The neurons are positioned in the map space, usually in a two-dimensional regular spacing in either a hexagonal or rectangular grid (instead of being organised in layers, like in feedforward ANNs).

During the training of a SOM, inputs are presented to the network and, for each, the weight vector closest to the input vector is identified. Then, the weights of all the neurons in the neighbourhood of the winner neuron (that is, the one whose weight vector was selected) are updated, so that they move towards the input vector. In this way, weight vectors of neighbouring neurons in the grid are likely to represent inputs which are close in the original feature space - a "topology-preserving" map is, thus, formed \cite{kohonen1995self, villmann1997topology}. An example of an application where SOM was used for FE can be found in \cite{lawrence1997face}, although many more are available in the literature.

\subsection{Radial Basis Function}

Radial Basis Function (RBF) neural networks are a type of ANNs characterised by their use of radial basis functions as activation functions in their neurons. Although these networks have been mainly used in classification problems, one can still find applications in the literature where RBF networks were used to perform feature extraction tasks, such as in \cite{lowe1997neuroscale}.

Similarly to Self-Organising Maps, Radial Basis Function neural networks explore neighborhood relationships in the data. When applied to FE tasks, RBF networks are used to predict the coordinates of the data points in the transformed feature space. Such predictions are computed during the training of these structures, by adjusting their weights in order to minimise a suitable error measure.

In short, a RBF network is composed of three neuron layers: input, hidden and output. It is common to adopt non-linear activation functions for the neurons in the network's hidden layer, such as a Gaussian function. On the other hand, neurons in the output layer typically implement a simple linear function, performing a weighted sum of their inputs.

According to Lowe and Tipping\cite{lowe1997neuroscale}, the training process of a RBF network is \emph{"relatively supervised"}. The main reason for this claim is related with the fact that, for each input sample, the specific representation in the transformed feature space is unknown; however a relative measure of the target separation between the points in the transformed space is provided.

\section{Conclusion}\label{sec:conclusion}

The current document presents a summarised overview of Feature Extraction methods exploring the application of Artificial Neural Networks.

Of the techniques included in this document, the application of stacked autoencoders and Convolutional Neural Networks has regained popularity among the \emph{deep learning} scientific community, mostly as a result of the existent computational capabilities of today's computers and machines; Other techniques, such as Self-Organising Maps and Radial Basis Function networks do not seem to be so popular in this community at the present.

Studying the available literature, additional methods can be found which are not covered in this document. Based on the researched literature the most cited and relevant methods were chosen to be covered in this document, as space and time limitations do not allow for a more extensive coverage of this area of research. Such analysis can be the subject of future work.

\bibliographystyle{ieeetr}
\bibliography{bibliography.bib}

\end{document}